{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아이펠톤에서 사용할 텍스트 생성 모델 테스트\n",
    "\n",
    "##### 기반지식\n",
    "- Text Generation task : 주어진 입력에 따라 텍스트를 이어서 생성하는 작업\n",
    "\n",
    "##### 코드 흐름\n",
    "- 허깅페이스에서 텍스트 생성쪽에 좋은 성능을 내는 모델을 선별\n",
    "    - **멘토님 추천** 최근 연구는 라마 3.1 8b를 기준으로 사용함. GPU 자원이 없으면 3b, 잘안되면 Qwen 2.5 모델\n",
    "        - IFEVAL 지표 기준으로 찾기 : 지시에 맞춰 잘 생성하는가 (70점 넘어야 적절, 안되도 60후반) \n",
    "    - [open_llm_leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) 에서 \n",
    "        - 논리적 일관성과 문맥 유지를 잘하는 모델을 찾기 위한 지표\n",
    "            - Perplexity (PPL) : 모델이 텍스트를 얼마나 잘 예측하는지를 나타내는 지표, 낮을 수록 좋음 \n",
    "            - BLEU (Bilingual Evaluation Understudy) : 생성된 텍스트가 참조 텍스트(즉, 정답 텍스트)와 얼마나 일치하는지를 측정, 좊을 수록 좋음 - 이건 우선순위가 아닐듯 \n",
    "            - ROUGE (Recall-Oriented Understudy for Gisting Evaluation) : 모델의 출력 텍스트와 참조 텍스트 간의 유사도를 평가하는 지표. 요약 작업에서 많이 사용되며, 일관성 있는지 측정 가능?, 높을 수록 좋음 \n",
    "            - Human Evaluation 지표도 존재하는 듯 (일관성, 창의성, 자연스러움 등)\n",
    "    - [text generation task](https://huggingface.co/models?pipeline_tag=text-generation&sort=likes) 에서 인기 좋은 것\n",
    "    - [챗봇 arena의 leaderboad](https://lmarena.ai/)에서 허깅페이스에 오픈된모델 \n",
    "        리더보드 점수는 **쌍대 비교(pairwise comparison)**를 기반으로 매겨집니다. 이 방식에서 두 모델의 답변을 비교한 후, 사용자가 더 나은 답변을 선택하는 방식으로 모델의 성능을 평가 -> 특정 task가 아닌 전반전인 성능을 사람이 선호하는 정도에 따라 결정됨\n",
    "    - [task > text-generation](https://huggingface.co/tasks/text-generation) 의 글 참고\n",
    "- (옵션) paperswithocde 에서 모델 선별\n",
    "- 모델 다운로드\n",
    "- 프롬프트 프리셋 생성, 변수 set 세팅\n",
    "- n개 테스트로 생성\n",
    "- OpenAI API 이용하여 평가 - 라이브러리 충돌되면 이건 다른 venv 코드에서 해야 할 수도 있음\n",
    "- 비교할 수 있게 엑셀로 만들기\n",
    "\n",
    "1차적으로 모델, 프롬프트 테스트를 위해 간단히 만들어봄 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 생성하는 파트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# CUDA가 사용 가능한지 확인하여 device 설정\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "# 출력해서 현재 선택된 device 확인\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# .env 파일에서 환경 변수 로드\n",
    "load_dotenv()\n",
    "# .env 파일에서 Hugging Face 토큰 불러오기\n",
    "token = os.getenv('HUGGINGFACE_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 모델 이름에 해당하는 파라미터 불러오기\n",
    "def get_model_parameters(row):  \n",
    "    params = {\n",
    "        'max_new_tokens': int(row['max_new_tokens']) if pd.notna(row['max_new_tokens']) else None,\n",
    "        'temperature': row['temperature'] if pd.notna(row['temperature']) else None,\n",
    "        'top_p': row['top-p'] if pd.notna(row['top-p']) else None,\n",
    "        'top_k': int(row['top-k']) if pd.notna(row['top-k']) else None,\n",
    "        'repetition_penalty': row['repetition_penalty'] if pd.notna(row['repetition_penalty']) else None\n",
    "    }\n",
    "\n",
    "    # None 값을 제거하여 기본값을 사용하게 함\n",
    "    params = {k: v for k, v in params.items() if v is not None}\n",
    "    \n",
    "    return params\n",
    "\n",
    "# 모델 결과를 출력하는 함수\n",
    "def get_result(row, prompt):\n",
    "    # 모델 파라미터 불러오기\n",
    "    params = get_model_parameters(row)\n",
    "    \n",
    "    # 모델 이름 불러오기\n",
    "    model_name = row['model_name']\n",
    "\n",
    "    # 모델 불러오기 (huggingface pipeline)\n",
    "    model = pipeline('text-generation', model=model_name, device=0)  # 'device=0'은 GPU 사용\n",
    "\n",
    "    # 파라미터 적용해서 결과 생성 (파라미터가 있을 경우만 전달)\n",
    "    result = model(prompt, **params)\n",
    "    \n",
    "    return result[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 설정\n",
    "## 파일 이름\n",
    "raw_filename = 'raw_data/meta0911.csv'\n",
    "prompt_text_list_filename = 'config/text_prompt_lst.csv' # 프롬프트 raw 텍스트 목록\n",
    "model_parameter_filename = 'config/model_parameter.csv'# 모델 파라미터 목록\n",
    "prompt_filename = 'config/prompt_list.csv' # 최종 프롬프트 목록\n",
    "gen_result_filename = 'data/meta_gen01.csv' # 생성 결과 파일 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['persona', 'pattern', 'pattern_def', 'thought', 'scenario',\n",
      "       'persona_in_scenario', 'thought_in_scenario'],\n",
      "      dtype='object')\n",
      "--------------------------\n",
      "                                              prompt\n",
      "0  Write a 4-6 sentence paragraph based on the fo...\n",
      "--------------------------\n",
      "                         model_name  max_new_tokens  temperature  top-p  \\\n",
      "0  meta-llama/Llama-3.2-1B-Instruct            1024          NaN    NaN   \n",
      "1  meta-llama/Llama-3.2-1B-Instruct             512          NaN    NaN   \n",
      "\n",
      "   top-k  repetition_penalty                                    unique_name  \n",
      "0    NaN                 NaN  meta-llama/Llama-3.2-1B-Instruct_1024_N_N_N_N  \n",
      "1    NaN                 NaN   meta-llama/Llama-3.2-1B-Instruct_512_N_N_N_N  \n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "raw_data = pd.read_csv(raw_filename, encoding='latin1')\n",
    "print(raw_data.columns)\n",
    "print(\"--------------------------\")\n",
    "\n",
    "prompt_text_lst = pd.read_csv(prompt_text_list_filename)\n",
    "print(prompt_text_lst.head(1))\n",
    "print(\"--------------------------\")\n",
    "\n",
    "model_list = pd.read_csv(model_parameter_filename)\n",
    "# print(model_list.head(2), type(model_list))\n",
    "\n",
    "# 기존에 'unique_name' 열이 있으면 삭제\n",
    "if 'unique_name' in model_list.columns:\n",
    "    model_list.drop(columns=['unique_name'], inplace=True)\n",
    "\n",
    "# 각 행마다 컬럼 데이터를 \"_\"로 연결, 값이 없으면 \"N\" 대체\n",
    "# 모든 데이터를 문자열로 변환한 후 결합\n",
    "model_list['unique_name'] = model_list.fillna('N').astype(str).agg('_'.join, axis=1)\n",
    "\n",
    "print(model_list.head(2))\n",
    "model_list.to_csv(model_parameter_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하나만 생성해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 불러오기 (huggingface pipeline)\n",
    "# model = pipeline('text-generation', model='meta-llama/Llama-3.2-1B-Instruct', device=0)  # 'device=0'은 GPU 사용\n",
    "\n",
    "# params = get_model_parameters(model_list.iloc[0])\n",
    "# print(params)\n",
    "\n",
    "# # 파라미터 적용해서 결과 생성 (파라미터가 있을 경우만 전달)\n",
    "# print(model(\"I want you to write an paragraph including 4 to 6 sentences in the form of diary of individual with mental issues.\"\n",
    "#             \"please include {given sentence} as it is, and make the paragraph to feel {emotion}.\"\n",
    "#             \"given sentence: I'm a vegan, and the restaurant served me a dish with fish in it. \"\n",
    "#             \"given sentence: They're trying to kill me. \"\n",
    "#             \"emotion: nervous\",\n",
    "#             **params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프롬프트 조합을 미리 저장해둠 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 프롬프트 구조에 맞춰 프롬프트 리스트 생성 \n",
    "model_lst = model_list.model_name.tolist()\n",
    "\n",
    "thought_list = raw_data['thought'].tolist()\n",
    "emtion_list = ['Depression', 'anger', 'anxiety', 'disappointment', 'helplessness']\n",
    "\n",
    "# 테스트용 프롬프트 리스트\n",
    "prompt_raw_list = prompt_text_lst['prompt'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테스트 끝나면 지울 부분 (아래 하나만)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thought_list = raw_data['thought'].tolist()[0:20]\n",
    "len(thought_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         model_name  max_new_tokens  temperature  top-p  \\\n",
      "0  meta-llama/Llama-3.2-1B-Instruct            1024          NaN    NaN   \n",
      "\n",
      "   top-k  repetition_penalty                                    unique_name  \\\n",
      "0    NaN                 NaN  meta-llama/Llama-3.2-1B-Instruct_1024_N_N_N_N   \n",
      "\n",
      "                                          raw_prompt  \\\n",
      "0  Write a 4-6 sentence paragraph based on the fo...   \n",
      "\n",
      "                                             thought     emotion  \\\n",
      "0  I like my cats. I think one day they will plot...  Depression   \n",
      "\n",
      "                                              prompt  \n",
      "0  Write a 4-6 sentence paragraph based on the fo...  \n"
     ]
    }
   ],
   "source": [
    "# 다양한 프롬프트 조합을 미리 저장해둠 \n",
    "rows = []\n",
    "\n",
    "for index, row in model_list.iterrows():\n",
    "    # 데이터프레임의 한 row 전체를 가져옴\n",
    "    for prompt in prompt_raw_list:\n",
    "        for thought in thought_list:\n",
    "            for emotion in emtion_list:\n",
    "                use_prompt = prompt.format(distorted_thought=thought, emotion=emotion)\n",
    "                \n",
    "                # row 자체에 추가할 데이터를 임시로 만듦\n",
    "                row_data = row.copy()  # 원본 row는 건드리지 않고 복사해서 사용\n",
    "                row_data['raw_prompt'] = prompt\n",
    "                row_data['thought'] = thought\n",
    "                row_data['emotion'] = emotion\n",
    "                row_data['prompt'] = use_prompt\n",
    "                \n",
    "                # 각 행을 리스트에 추가\n",
    "                rows.append(row_data)\n",
    "\n",
    "# 리스트를 데이터프레임으로 변환\n",
    "prompt_list_df = pd.DataFrame(rows)\n",
    "\n",
    "# 결과 저장\n",
    "prompt_list_df.to_csv(prompt_filename, index=False)\n",
    "print(prompt_list_df.head(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장해둔 프롬프트 조합을 불러와서 n개씩 결과 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장해둔 프롬프트 조합을 불러와서 n개씩 결과 생성 \n",
    "\n",
    "## 데이터 로드\n",
    "prompt_list_df = pd.read_csv(prompt_filename)\n",
    "\n",
    "## 생성 결과 데이터 가져와서, 이미 생성된 결과는 제외 (result 컬럼에 값이 있으면 제외)\n",
    "if os.path.exists(gen_result_filename):\n",
    "    gen_result = pd.read_csv(gen_result_filename)\n",
    "    \n",
    "    # n 개의 컬럼이 모두 중복되는 경우 중복 제거\n",
    "    merged = pd.merge(prompt_list_df, gen_result[['prompt', 'unique_name']], on=['prompt', 'unique_name'], how='left', indicator=True)\n",
    "    \n",
    "    # '_merge' 컬럼이 'left_only'인 데이터만 남김 (즉, 중복되지 않은 데이터)\n",
    "    prompt_list_df = merged[merged['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "else:\n",
    "    # 파일이 없으면 새로운 데이터프레임을 생성\n",
    "    gen_result = pd.DataFrame(columns=[*prompt_list_df.columns, 'result', 'time'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 하나씩 생성, 모델 테스트용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:623: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "C:\\Users\\saink\\AppData\\Local\\Temp\\ipykernel_49480\\3760092767.py:57: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  gen_result = pd.concat([gen_result, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 1700\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 1500\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 2100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: chuanli11/Chat-Llama-3.2-3B-Instruct-uncensored, index: 1300\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "chuanli11/Chat-Llama-3.2-3B-Instruct-uncensored is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/chuanli11/Chat-Llama-3.2-3B-Instruct-uncensored/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1339\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1338\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[1;32m-> 1339\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1854\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[0;32m   1853\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1854\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1856\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1746\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1746\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1747\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1666\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1665\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1666\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1674\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1675\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:364\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 364\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:388\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    387\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 388\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m     )\n\u001b[1;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-6703a435-0eae3961514fa18c75372d8a;12ffc7aa-9c99-4c4e-8139-b50490b5b6d2)\n\nRepository Not Found for url: https://huggingface.co/chuanli11/Chat-Llama-3.2-3B-Instruct-uncensored/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m prompt \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     46\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# 시작 시간 기록\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     48\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# 종료 시간 기록\u001b[39;00m\n\u001b[0;32m     50\u001b[0m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m result  \u001b[38;5;66;03m# 생성된 결과 저장\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 25\u001b[0m, in \u001b[0;36mget_result\u001b[1;34m(row, prompt)\u001b[0m\n\u001b[0;32m     22\u001b[0m model_name \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 모델 불러오기 (huggingface pipeline)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 'device=0'은 GPU 사용\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 파라미터 적용해서 결과 생성 (파라미터가 있을 경우만 전달)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m result \u001b[38;5;241m=\u001b[39m model(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:768\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m     pretrained_model_name_or_path \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig) \u001b[38;5;129;01mand\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[1;32m--> 768\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    777\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:426\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    436\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    437\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: chuanli11/Chat-Llama-3.2-3B-Instruct-uncensored is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "# 하나씩 생성, 모델 테스트용\n",
    "\n",
    "# unique_name = 'meta-llama_Llama-3.2-1B-Instruct' 으로 변경\n",
    "\n",
    "model_lst = [\n",
    "            # 'meta-llama/Llama-3.2-1B-Instruct',\n",
    "            # 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "            # 'Qwen/Qwen2.5-7B-Instruct',\n",
    "            # 'meta-llama/Llama-3.1-8B-Instruct', \n",
    "            # 'meta-llama/Llama-3.1-70B-Instruct',\n",
    "            # 'meta-llama/Llama-3.1-405B-Instruct',\n",
    "            #  \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "            # \"mistralai/Mixtral-8x22B-Instruct-v0.1\",\n",
    "            #  \"Qwen/Qwen2.5-7B-Instruct\", \"Qwen/Qwen2.5-14B-Instruct\", \"Qwen/Qwen2.5-32B-Instruct\", \"Qwen/Qwen2.5-72B-Instruct\",\n",
    "             ]\n",
    "\n",
    "\n",
    "unique_lst = [\n",
    "            # 'meta-llama/Llama-3.2-1B-Instruct_1024.0_N_1.0_20.0_1.2',\n",
    "            'meta-llama/Llama-3.2-1B-Instruct_1024_N_N_N_N',\n",
    "]\n",
    "\n",
    "## 모델별로 n개의 결과 생성\n",
    "n = 1\n",
    "\n",
    "# 모델,raw prompt 별로 n개의 row를 선택하고 반복문 실행\n",
    "for (unique_name, raw_prompt), group in prompt_list_df.groupby(['unique_name', 'raw_prompt']):\n",
    "    # 각 모델 그룹에서 최대 n개의 행만 선택\n",
    "    model_rows = group.head(n)\n",
    "    \n",
    "    model_name = model_rows.iloc[0]['model_name']\n",
    "    \n",
    "    # print(model_rows.head(2))\n",
    "    # print(model_name)\n",
    "    \n",
    "    # 특정 모델만 필터링\n",
    "    if unique_name not in unique_lst:\n",
    "        print(f\"Skipping model: {model_name}\")\n",
    "        continue\n",
    "    \n",
    "    # 반복문 내에서 필요한 작업 수행\n",
    "    for index, row in model_rows.iterrows():\n",
    "        try:\n",
    "            print(f\"Model: {model_name}, index: {index}\")\n",
    "            prompt = row['prompt']\n",
    "            \n",
    "            start_time = time.time() # 시작 시간 기록\n",
    "            result = get_result(row, prompt) \n",
    "            end_time = time.time() # 종료 시간 기록\n",
    "            \n",
    "            row['result'] = result  # 생성된 결과 저장\n",
    "            row['time'] = round(end_time - start_time, 0) # 실행 시간 저장\n",
    "\n",
    "            # 새로 생성된 데이터를 result_data에 추가\n",
    "            new_row = pd.DataFrame([row])\n",
    "                \n",
    "            # pd.concat을 사용하여 새로운 데이터를 result_data에 추가\n",
    "            gen_result = pd.concat([gen_result, new_row], ignore_index=True)\n",
    "            \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            gen_result.to_csv('data/test00.csv', index=False)\n",
    "        \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# 생성된 데이터를 파일에 다시 저장 (이전 결과와 함께)\n",
    "gen_result.to_csv('data/test00.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_result.to_csv('data/test00.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단위별로 n개의 결과 생성\n",
    "n = 3\n",
    "\n",
    "# 모델,raw prompt 별로 n개의 row를 선택하고 반복문 실행\n",
    "for (unique_name, raw_pgptrompt), group in prompt_list_df.groupby(['unique_name', 'raw_prompt']):\n",
    "    # 각 모델 그룹에서 최대 n개의 행만 선택\n",
    "    model_rows = group.head(n)\n",
    "    \n",
    "    model_name = model_rows.iloc[0]['model_name']\n",
    "    \n",
    "    # 반복문 내에서 필요한 작업 수행\n",
    "    for index, row in model_rows.iterrows():\n",
    "        try:\n",
    "            print(f\"Model: {model_name}, index: {index}\")\n",
    "            prompt = row['prompt']\n",
    "            \n",
    "            start_time = time.time() # 시작 시간 기록\n",
    "            result = get_result(row, prompt) \n",
    "            end_time = time.time() # 종료 시간 기록\n",
    "            \n",
    "            row['result'] = result  # 생성된 결과 저장\n",
    "            row['time'] = round(end_time - start_time, 0) # 실행 시간 저장\n",
    "\n",
    "            # 새로 생성된 데이터를 result_data에 추가\n",
    "            new_row = pd.DataFrame([row])\n",
    "                \n",
    "            # pd.concat을 사용하여 새로운 데이터를 result_data에 추가\n",
    "            gen_result = pd.concat([gen_result, new_row], ignore_index=True)\n",
    "            \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            # 파일이 없을 경우 헤더를 포함해 쓰고, 파일이 있을 경우 append 모드로 데이터 추가\n",
    "            if not os.path.exists(gen_result_filename):\n",
    "                gen_result.to_csv(gen_result_filename, mode='w', header=True, index=False)  # 파일 없을 때는 헤더 포함\n",
    "            else:\n",
    "                gen_result.to_csv(gen_result_filename, mode='a', header=False, index=False)  # 파일 있을 때는 헤더 제외\n",
    "        \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 파일이 없을 경우 헤더를 포함해 쓰고, 파일이 있을 경우 append 모드로 데이터 추가\n",
    "if not os.path.exists(gen_result_filename):\n",
    "    gen_result.to_csv(gen_result_filename, mode='w', header=True, index=False)  # 파일 없을 때는 헤더 포함\n",
    "else:\n",
    "    gen_result.to_csv(gen_result_filename, mode='a', header=False, index=False)  # 파일 있을 때는 헤더 제외"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 생성된 텍스트에 지시사항 준수 점수 부여\n",
    "\n",
    "- LLM의 답변에서 사족을 제거함 \n",
    "- 필수 포함 문장이 들어가 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\saink\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from difflib import SequenceMatcher\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_result_filename = 'data/meta_gen01.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              model  \\\n",
      "0  meta-llama/Llama-3.2-3B-Instruct   \n",
      "\n",
      "                                          raw_prompt  \\\n",
      "0  You are a patient receiving psychological coun...   \n",
      "\n",
      "                                             thought     emotion  \\\n",
      "0  I like my cats. I think one day they will plot...  Depression   \n",
      "\n",
      "                                              prompt  \\\n",
      "0  You are a patient receiving psychological coun...   \n",
      "\n",
      "                                              result        time  \n",
      "0  You are a patient receiving psychological coun...  105.448529  \n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "gen_result = pd.read_csv(gen_result_filename)\n",
    "print(gen_result.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 클리닝\n",
    "- 원하는 답변만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 우리가 원하는 답변만 추출하는 함수 \n",
    "# def extract_reponse(text):\n",
    "#     # --- 사이에 텍스트가 있는지 확인\n",
    "#     pattern_01 = r'---\\n(.*?)\\n---'\n",
    "#     matches_01 = re.findall(pattern_01, text, re.DOTALL)\n",
    "\n",
    "#     # --- 이후 텍스트 추출\n",
    "#     pattern_02 = r'---\\n(.*)'\n",
    "#     matches_02 = re.search(pattern_02, text, re.DOTALL)\n",
    "\n",
    "#     # 결과 반환\n",
    "#     if matches_01:\n",
    "#         result = matches_01[0]\n",
    "#     elif matches_02:\n",
    "#         result = matches_02.group(1)\n",
    "#     elif '\\n' not in text: # 줄바꿈이 없는 경우\n",
    "#         result = text\n",
    "#     elif '\\n' in text: # 줄바꿈이 있는 경우\n",
    "#         # ':' 뒤에 공백이 있으면 그 이전의 텍스트를 모두 제거함 (앞부분 사족은 대부분 이걸로 제거됨)-- 아이디어는 좋았는데, 다른 문장도 제거됨\n",
    "#         # pattern_03 = r'.*:\\s+'\n",
    "#         # matches_03 = re.sub(pattern_03, '', text, flags=re.DOTALL)\n",
    "\n",
    "#         # 특정 문자열을 포함한 문장 제거를 위한 키워드 리스트\n",
    "#         remove_keywords = [\"Sure,\", \"Here's the completion of the diary\", \"Here are your completed journal entries:\",\n",
    "#                            \"2. \",\n",
    "#                            \"I hope this helps\", \"Would you like to\", \"Does this help\", \"you would like to add or edit\", \"I hope this fills in the missing parts\",\n",
    "#                            ]\n",
    "#         # 텍스트를 문장 단위로 나누기\n",
    "#         sentences = text.split('\\n')\n",
    "#         # 여러 문자열 중 하나라도 포함된 문장 제거\n",
    "#         filtered_sentences = [sentence for sentence in sentences if not any(keyword in sentence for keyword in remove_keywords)]\n",
    "#         # 문장들을 다시 결합\n",
    "#         result = ' '.join(filtered_sentences)\n",
    "#     else:\n",
    "#         result = None \n",
    "\n",
    "#     # 잘못된 문법 정제\n",
    "#     if result is not None:\n",
    "#         result = result.strip().replace(\"?™\", \"'\").replace(\"I are\", \"I am\").replace(\"I were\", \"i am\").replace(\"1. \", \"\").strip('\"*')\n",
    "    \n",
    "#     return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 부분 추가해야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필수 문장이 잘들어있는지 지시사항 준수 비율 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 문장이 잘 들어 있는가? \n",
    "## 필수 문장 내 단어와 story의 문장 중에 단어가 겹치는 % 비율 계산\n",
    "## Longest Common Subsequence(LCS, 최장 공통 부분 문자열) 알고리즘을 사용하여 문장 유사도 측정\n",
    "\n",
    "class instruct_check_sentence_include:\n",
    "    # x = 데이터프레임\n",
    "    # compare_column = 비교할 문장 컬럼명 텍스트\n",
    "    # phragraph = 비교할 문단 컬럼명 텍스트\n",
    "    def __init__(self, x, compare_column, phragraph):\n",
    "        self.data = x\n",
    "        self.data[\"compare_sentences\"] = self.data[compare_column].str.replace(', ', '. ', case=False)\n",
    "        self.data[\"compare_sentences\"] = self.data[\"compare_sentences\"].apply(lambda x: nltk.sent_tokenize(x))\n",
    "        \n",
    "        self.data[\"phragraph_sentences\"] = self.data[phragraph].str.replace(', ', '. ', case=False)\n",
    "        self.data[\"phragraph_sentences\"] = self.data[\"phragraph_sentences\"].apply(lambda x: nltk.sent_tokenize(x))\n",
    "\n",
    "    ## 필수 문장 내 단어와 story의 문장 중에 단어가 겹치는 % 비율 계산\n",
    "    ### 문장 부호를 제거한 후, 소문자로 변환하고 단어로 분리하는 함수\n",
    "    def clean_and_split(self, sentence):\n",
    "        # 문장 부호 제거 (string.punctuation을 사용하여 기본적인 문장 부호 제거)\n",
    "        cleaned_sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "                \n",
    "        # 소문자로 변환하고 단어로 분리\n",
    "        words = cleaned_sentence.lower().split()\n",
    "        \n",
    "        return words\n",
    "\n",
    "    ### 두 문장 간의 공통 단어 비율을 계산하는 함수\n",
    "    def partial_inclusion_ratio(self, compare_sentence, phragraph) :\n",
    "        # 두 문장을 단어로 분리\n",
    "        compare_words = list(self.clean_and_split(compare_sentence))\n",
    "        phragraph_words = list(self.clean_and_split(phragraph))\n",
    "\n",
    "        # 공통 문자를 찾기 위해 base_sentence의 문자들이 sentence에 얼마나 포함되는지 확인\n",
    "        common_words = [word for word in compare_words if word in phragraph_words]\n",
    "        \n",
    "        # 포함된 문자 비율을 계산\n",
    "        if len(common_words) == 0:\n",
    "            inclusion_ratio = 0\n",
    "        else:\n",
    "            inclusion_ratio = len(common_words) / len(compare_words)\n",
    "        \n",
    "        return round(inclusion_ratio, 2)\n",
    "    \n",
    "    ## Longest Common Subsequence(LCS, 최장 공통 부분 문자열) 알고리즘을 사용하여 문장 유사도 측정\n",
    "    def similar(self, a, b):\n",
    "        return round(SequenceMatcher(None, a, b).ratio(), 2)\n",
    "    \n",
    "    ### 각 row의 모든 문장 쌍에 대해 유사도를 계산하는 함수\n",
    "    def calculate_all_ratios(self):\n",
    "        # 새로운 리스트에 각 row에 대해 유사도를 저장\n",
    "        compare_sentences_list = [] # 포함되어야 하는 문장\n",
    "        phragraph_sentences_list = [] # 공통단어비율 계산할 story 문장\n",
    "        row_similarity_ratios = []\n",
    "        min_ratios = []\n",
    "        \n",
    "        phragraph_LCS_sentences_list = []\n",
    "        row_LCS_similarity_ratios = []\n",
    "        min_LCS_ratios = []\n",
    "        \n",
    "        final_p_sen_list = []\n",
    "        final_max_ratio_list = []\n",
    "        final_min_ratio = []\n",
    "        \n",
    "        for index, row in self.data.iterrows():\n",
    "            compare_sentences = row['compare_sentences']\n",
    "            phragraph_sentences = row['phragraph_sentences']\n",
    "            \n",
    "            # 각 문장 쌍에 대해 partial_inclusion_ratio 계산\n",
    "            c_list = []\n",
    "            p_list = []\n",
    "            row_ratios = []\n",
    "            LCS_p_list = []\n",
    "            LCS_row_ratios = []\n",
    "            final_p_list = []\n",
    "            final_max_ratios = []\n",
    "            \n",
    "            for compare_sentence in compare_sentences:\n",
    "                max_ratio = 0\n",
    "                p_sen = ''\n",
    "                max_LCS_ratio = 0\n",
    "                LCS_p_sen = ''\n",
    "                final_p_sen = ''\n",
    "                final_max_ratio = 0\n",
    "                \n",
    "                # 문장이 너무 짧으면 비교하지 않음\n",
    "                if len(compare_sentence) <= 2:\n",
    "                    continue\n",
    "                \n",
    "                for phragraph_sentence in phragraph_sentences:\n",
    "                    # 문장이 포함관계이면, 공통 단어 비율을 1로 저장\n",
    "                    clean_c_sen = compare_sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "                    clean_p_sen = phragraph_sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "                    \n",
    "                    if clean_c_sen in clean_p_sen or clean_p_sen in clean_c_sen:\n",
    "                        max_ratio = 1.0\n",
    "                        p_sen = phragraph_sentence\n",
    "                        # continue\n",
    "                    else:\n",
    "                        # 공통 단어 비율 계산\n",
    "                        ratio = self.partial_inclusion_ratio(compare_sentence, phragraph_sentence)\n",
    "\n",
    "                        if ratio > max_ratio: \n",
    "                            max_ratio = ratio\n",
    "                            p_sen = phragraph_sentence\n",
    "                        \n",
    "                    # LCS 관점의 유사도 계산\n",
    "                    LCS_ratio = self.similar(compare_sentence, phragraph_sentence)\n",
    "                    \n",
    "                    # LCS 유사도가 더 높으면 업데이트\n",
    "                    if LCS_ratio > max_LCS_ratio:\n",
    "                        max_LCS_ratio = LCS_ratio\n",
    "                        LCS_p_sen = phragraph_sentence\n",
    "                        \n",
    "                    # 최종 문장 저장\n",
    "                    if max_ratio >= max_LCS_ratio:\n",
    "                        final_p_sen = p_sen\n",
    "                    else:\n",
    "                        final_p_sen = LCS_p_sen\n",
    "                        \n",
    "                    # 최종 유사도 저장\n",
    "                    if max_ratio >= max_LCS_ratio:\n",
    "                        final_max_ratio = max_ratio\n",
    "                    else:\n",
    "                        final_max_ratio = max_LCS_ratio\n",
    "                    \n",
    "                c_list.append(compare_sentence) # 필수 비교 문장 저장\n",
    "                p_list.append(p_sen) # 공통단어비율 문장 저장\n",
    "                row_ratios.append(max_ratio) # 공통단어비율 저장\n",
    "                LCS_p_list.append(LCS_p_sen) # LCS 문장 저장\n",
    "                LCS_row_ratios.append(max_LCS_ratio) # LCS 저장\n",
    "                final_p_list.append(final_p_sen) # 최종 문장 저장\n",
    "                final_max_ratios.append(final_max_ratio) # 최종 유사도 저장\n",
    "                    \n",
    "            # 한 row에서 가장 높은 유사도 문장과, 가장 낮은 유사도 저장\n",
    "            compare_sentences_list.append(c_list)\n",
    "            phragraph_sentences_list.append(p_list)\n",
    "            row_similarity_ratios.append(row_ratios)\n",
    "            min_ratios.append(min(row_ratios) if row_ratios else 0)\n",
    "            phragraph_LCS_sentences_list.append(LCS_p_list)\n",
    "            row_LCS_similarity_ratios.append(LCS_row_ratios)\n",
    "            min_LCS_ratios.append(min(LCS_row_ratios) if LCS_row_ratios else 0)\n",
    "            final_p_sen_list.append(final_p_list)\n",
    "            final_max_ratio_list.append(final_max_ratios)\n",
    "            final_min_ratio.append(min(final_max_ratios) if final_max_ratios else 0)\n",
    "\n",
    "        # 데이터프레임에 결과 추가\n",
    "        self.data['compare_sentences'] = compare_sentences_list\n",
    "        self.data['phragraph_sentences'] = phragraph_sentences_list\n",
    "        self.data['공통단어비율'] = row_similarity_ratios\n",
    "        self.data['min_공통단어비율'] = min_ratios\n",
    "        self.data['LCS_sentences'] = phragraph_LCS_sentences_list\n",
    "        self.data['LCS_유사도'] = row_LCS_similarity_ratios\n",
    "        self.data['min_LCS_유사도'] = min_LCS_ratios\n",
    "        self.data['final_p_sen'] = final_p_sen_list\n",
    "        self.data['final_max_ratio'] = final_max_ratio_list\n",
    "        self.data['final_min_ratio'] = final_min_ratio\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              model  \\\n",
      "0  meta-llama/Llama-3.2-3B-Instruct   \n",
      "\n",
      "                                          raw_prompt  \\\n",
      "0  You are a patient receiving psychological coun...   \n",
      "\n",
      "                                             thought     emotion  \\\n",
      "0  I like my cats. I think one day they will plot...  Depression   \n",
      "\n",
      "                                              prompt  \\\n",
      "0  You are a patient receiving psychological coun...   \n",
      "\n",
      "                                              result        time  \\\n",
      "0  You are a patient receiving psychological coun...  105.448529   \n",
      "\n",
      "                                   compare_sentences  \\\n",
      "0  [I like my cats., I think one day they will pl...   \n",
      "\n",
      "                                 phragraph_sentences  \n",
      "0  [You are a patient receiving psychological cou...  \n",
      "Index(['model', 'raw_prompt', 'thought', 'emotion', 'prompt', 'result', 'time',\n",
      "       'compare_sentences', 'phragraph_sentences'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "aft_fnd_instrct_flnm = 'data/meta_gen01_find.csv'\n",
    "\n",
    "a = instruct_check_sentence_include(gen_result, 'thought', 'result')\n",
    "b = a.calculate_all_ratios()\n",
    "\n",
    "aft_fnd_instrct = b.to_csv(aft_fnd_instrct_flnm, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 생성된 데이터셋에 공통된 문장 표현이 존재하는가? - 중복 문장 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임의 story 컬럼에서 문장을 추출하고, 빈도를 계산하는 함수\n",
    "def count_common_sentences(df, story_column, clean = False):\n",
    "    # 모든 story 컬럼의 텍스트를 가져옴\n",
    "    all_sentences = []\n",
    "    \n",
    "    for story in df[story_column]:\n",
    "        if clean:\n",
    "            # , 를 . 로 바꿔서 문장을 더 잘게 나눔\n",
    "            story = story.replace(', ', '. ')\n",
    "        \n",
    "        # 문장을 분리 (원문 그대로)\n",
    "        sentences = nltk.sent_tokenize(story)\n",
    "        \n",
    "        # 모든 문장을 리스트에 추가\n",
    "        all_sentences.extend(sentences)\n",
    "    \n",
    "    # 각 문장의 빈도 계산\n",
    "    sentence_counts = Counter(all_sentences)\n",
    "    \n",
    "    # 빈도별로 내림차순 정렬된 결과 반환\n",
    "    return sentence_counts.most_common()\n",
    "\n",
    "# 공통 문장에서 필수 문장 제거 함수\n",
    "def exclude_should_thought(common_sentences, should_sentences):\n",
    "    # should_thought의 문장만 추출\n",
    "    should_thought_sentences = {sentence for sentence, _ in should_sentences}\n",
    "    \n",
    "    # common_sentences에서 should_thought에 없는 문장만 필터링\n",
    "    filtered_common_sentences = [(sentence, count) for sentence, count in common_sentences if sentence not in should_thought_sentences]\n",
    "    \n",
    "    return filtered_common_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['model', 'raw_prompt', 'thought', 'emotion', 'prompt', 'result',\n",
      "       'time'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "gen_result_filename = 'data/meta_gen01.csv'\n",
    "\n",
    "gen_result = pd.read_csv(gen_result_filename)\n",
    "print(gen_result.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'data/cmmn_sentnc_chck_0.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m sentence_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([tmp_01, tmp_02, tmp_03, tmp_04, tmp_05, tmp_06], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 저장\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[43msentence_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/cmmn_sentnc_chck_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m    \n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# 모델명, 프롬프트명 기록\u001b[39;00m\n\u001b[0;32m     37\u001b[0m num_lst\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'data/cmmn_sentnc_chck_0.csv'"
     ]
    }
   ],
   "source": [
    "# 그룹핑한 데이터를 딕셔너리 형태로 변환\n",
    "grouped = gen_result.groupby(['model', 'raw_prompt'])\n",
    "\n",
    "i = 0\n",
    "mdl_nm_lst = []\n",
    "raw_prmpt_lst = []\n",
    "num_lst = []\n",
    "\n",
    "# 각 그룹마다 문장 추출 및 필터링 적용\n",
    "for model_name, raw_prompt in grouped:\n",
    "    # 각 그룹에서 필수로 포함해야 하는 문장 추출\n",
    "    should_thought = count_common_sentences(gen_result, 'thought', False)\n",
    "    clean_should_thought = count_common_sentences(gen_result, 'thought', True)\n",
    "    \n",
    "    # 생성한 텍스트들에서 공통 문장 추출\n",
    "    common_sentences = count_common_sentences(gen_result, 'result', False)\n",
    "    clean_common_sentences = count_common_sentences(gen_result, 'result', True)\n",
    "\n",
    "    # 생성한 텍스트 공통 문장 - 필수로 포함해야 하는 문장\n",
    "    filtered_common_sentences = exclude_should_thought(common_sentences, should_thought)\n",
    "    filtered_clean_common_sentences = exclude_should_thought(clean_common_sentences, clean_should_thought)\n",
    "    \n",
    "    # 데이터프레임 양옆으로 합치기\n",
    "    tmp_01 = pd.DataFrame(should_thought, columns=['thought_공통문장', 'Count'])\n",
    "    tmp_02 = pd.DataFrame(common_sentences, columns=['gen_공통문장', 'Count'])\n",
    "    tmp_03 = pd.DataFrame(filtered_common_sentences, columns=['filtered_공통문장', 'Count'])\n",
    "    tmp_04 = pd.DataFrame(clean_should_thought, columns=['thought_공통문장', 'Count'])\n",
    "    tmp_05 = pd.DataFrame(clean_common_sentences, columns=['gen_공통문장', 'Count'])\n",
    "    tmp_06 = pd.DataFrame(filtered_clean_common_sentences, columns=['filtered_공통문장', 'Count'])\n",
    "\n",
    "    sentence_df = pd.concat([tmp_01, tmp_02, tmp_03, tmp_04, tmp_05, tmp_06], axis=1)\n",
    "    \n",
    "    # 저장\n",
    "    sentence_df.to_csv(f'data/cmmn_sentnc_chck_{i}.csv', index=False)    \n",
    "    \n",
    "    # 모델명, 프롬프트명 기록\n",
    "    num_lst.append(i)\n",
    "    mdl_nm_lst.append(model_name)\n",
    "    raw_prmpt_lst.append(raw_prompt)\n",
    "    \n",
    "    # 반복문 카운트\n",
    "    i += 1\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame({\n",
    "    'num': num_lst,\n",
    "    'model': mdl_nm_lst,\n",
    "    'raw_prompt': raw_prmpt_lst,\n",
    "})\n",
    "\n",
    "# 저장\n",
    "df.to_csv('data/cmmn_sentnc_chck_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 생성된 텍스트를 OpenAi GPT 4o를 이용하여 평가 점수 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"API_KEY\")\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api함수 \n",
    "def get_chatgpt_response(input_01, input_02):\n",
    "    # OpenAI API를 통해 ChatGPT에게 한국어로 자연스럽게 다듬어 달라고 요청\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        # model=\"gpt-4o-mini\",\n",
    "        # model =\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a patient receiving psychological counseling. who can speak english only.\"},\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": f\"다음에 제공되는 2 문장은 너가 쓴 일기인데, 2~3문장이 빠져 있어 {input_01} {input_02}, 주어진 문장을 그대로 변경 없이 포함해서, 비어있는 내용을 영어로 써줘\"}\n",
    "        ]\n",
    "    )\n",
    "    # return response['choices'][0]['message']['content']\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['model', 'raw_prompt', 'thought', 'emotion', 'prompt', 'result', 'time',\n",
      "       'compare_sentences', 'phragraph_sentences', '공통단어비율', 'min_공통단어비율',\n",
      "       'LCS_sentences', 'LCS_유사도', 'min_LCS_유사도', 'final_p_sen',\n",
      "       'final_max_ratio', 'final_min_ratio'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "aft_fnd_instrct_flnm = 'data/meta_gen01_find.csv'\n",
    "\n",
    "aft_fnd_instrct = pd.read_csv(aft_fnd_instrct_flnm)\n",
    "print(aft_fnd_instrct.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
